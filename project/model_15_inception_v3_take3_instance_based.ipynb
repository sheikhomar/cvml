{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model-15-inception-v3-take3-instance-based.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1zEiTFbHptlN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ff3206d-fed5-4b49-9aca-d5f378b98c89",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525882001111,
          "user_tz": -120,
          "elapsed": 563,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "ZtoUYEMwp1-I",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebda8370-e4f5-4705-9006-33eb665b3034",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525882024743,
          "user_tz": -120,
          "elapsed": 1525,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdatalab\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ccgR04Fqp-z_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "552e9e69-ec2f-4cda-8a84-bbdec161527b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525882059226,
          "user_tz": -120,
          "elapsed": 30134,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sheikhomar/cvml.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cvml'...\n",
            "remote: Counting objects: 78328, done.\u001b[K\n",
            "remote: Total 78328 (delta 0), reused 0 (delta 0), pack-reused 78328\u001b[K\n",
            "Receiving objects: 100% (78328/78328), 476.18 MiB | 30.99 MiB/s, done.\n",
            "Resolving deltas: 100% (736/736), done.\n",
            "Checking out files: 100% (77993/77993), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T1C5YzClqAP2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1f06341-2333-479a-c3f4-f758102c4cf7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525882059733,
          "user_tz": -120,
          "elapsed": 483,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "cd cvml/project/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cvml/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f23i9Ho4qB64",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0996873f-c658-444a-8127-f0d86e2a6d6b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525882079874,
          "user_tz": -120,
          "elapsed": 488,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "cd saved_weights/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/cvml/project/saved_weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cfWO4KGiqLAW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "321c3978-7551-4ccd-e2b1-3518aba64fc0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525883001595,
          "user_tz": -120,
          "elapsed": 756898,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b1f4fa94-abce-49d6-a2c1-855b96deeb66\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b1f4fa94-abce-49d6-a2c1-855b96deeb66\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5 to model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5\n",
            "User uploaded file \"model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5\" with length 90047632 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0wkeAz5ZtOFE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "64fae40a-94e3-4806-a5f6-128c5460a1c6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525883009368,
          "user_tz": -120,
          "elapsed": 1507,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5\r\n",
            "myfile.bin\r\n",
            "sentinel\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ZFAQ0b_q04c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import urllib.request\n",
        "import h5py\n",
        "\n",
        "from keras import applications\n",
        "from keras.models import Model\n",
        "from keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def preprocess_input(x):\n",
        "    x /= 255.\n",
        "    x -= 0.5\n",
        "    x *= 2.\n",
        "    return x\n",
        "\n",
        "\n",
        "class ModelBase:\n",
        "    def __init__(self,\n",
        "                 model_name=None,\n",
        "                 batch_size=16,\n",
        "                 verbose=0,\n",
        "                 n_freeze_layers=0,\n",
        "                 learning_rate=0.00001,\n",
        "                 epochs=400,\n",
        "                 optimizer='adam'\n",
        "                 ):\n",
        "        if model_name is None:\n",
        "            script_name, script_ext = os.path.splitext(sys.argv[0])\n",
        "            self.model_name = os.path.basename(script_name)\n",
        "        else:\n",
        "            self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.train_data_dir = \"Train/TrainImages\"\n",
        "        self.validation_data_dir = \"Validation/ValidationImages\"\n",
        "        self.test_data_dir = \"Test/TestImages\"\n",
        "        self.img_width = 256\n",
        "        self.img_height = 256\n",
        "        self.img_channels = 3\n",
        "        self.n_train_samples = 5830\n",
        "        self.n_validation_samples = 2298\n",
        "        self.n_test_samples = 3460\n",
        "        self.n_labels = 29\n",
        "        self.epochs = epochs\n",
        "        self.n_freeze_layers = n_freeze_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.imagenet_weights_url = None\n",
        "        self.imagenet_use_id = False\n",
        "\n",
        "    def load_model(self, model_weights=None):\n",
        "        print('Creating model...')\n",
        "        self._create()\n",
        "        print('Loading weights from {}...'.format(model_weights))\n",
        "        self.model.load_weights(model_weights)\n",
        "        print('Compiling...')\n",
        "        sgd = optimizers.SGD(lr=self.learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "        adam = optimizers.Adam(lr=self.learning_rate)\n",
        "        nadam = optimizers.Nadam(lr=self.learning_rate)\n",
        "        self.model.compile(\n",
        "            #optimizers.Adam(lr=self.learning_rate),\n",
        "            #optimizers.Nadam(),\n",
        "            optimizers.SGD(momentum=0.9),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return self.model\n",
        "\n",
        "    def train(self):\n",
        "        print('Creating model...')\n",
        "        self._create()\n",
        "\n",
        "        self._freeze_top_layers()\n",
        "\n",
        "        print('Loading weights...')\n",
        "        self._load_pretrained_weights()\n",
        "\n",
        "        print('Compiling...')\n",
        "        self.model.compile(\n",
        "            #optimizers.Adam(lr=self.learning_rate),\n",
        "            #optimizers.Nadam(),\n",
        "            optimizers.SGD(momentum=0.9),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        #print(self.model.summary())\n",
        "\n",
        "        # Data generators for the model\n",
        "        train_gen = self._get_train_generator()\n",
        "        validation_gen = self._get_validation_generator()\n",
        "\n",
        "        print('Training model...')\n",
        "        self.model.fit_generator(\n",
        "            train_gen,\n",
        "            steps_per_epoch=int(self.n_train_samples / self.batch_size),\n",
        "            validation_data=validation_gen,\n",
        "            validation_steps=int(self.n_validation_samples / self.batch_size),\n",
        "            epochs=self.epochs,\n",
        "            callbacks=self._get_callbacks(),\n",
        "            verbose=self.verbose\n",
        "        )\n",
        "\n",
        "    def predict_validation(self, model_weights):\n",
        "        img_paths = self._get_validation_image_paths()\n",
        "        return self._predict(model_weights, img_paths)\n",
        "\n",
        "    def predict_test(self, model_weights):\n",
        "        img_paths = self._get_test_image_paths()\n",
        "        return self._predict(model_weights, img_paths)\n",
        "\n",
        "    def predict_instance_validation(self, model_weights):\n",
        "        img_paths = self._get_validation_image_paths()\n",
        "        return self._predict_instance(model_weights, img_paths)\n",
        "      \n",
        "    def predict_instance_test(self, model_weights):\n",
        "        img_paths = self._get_test_image_paths()\n",
        "        return self._predict_instance(model_weights, img_paths)\n",
        "      \n",
        "    @staticmethod\n",
        "    def write_predictions(predictions, file_name='predictions.csv'):\n",
        "        with open(file_name, 'w') as file:\n",
        "            file.write('ID,Label')\n",
        "            for index, value in enumerate(predictions):\n",
        "                file.write('\\n{0},{1}'.format(index + 1, value))\n",
        "\n",
        "    @staticmethod\n",
        "    def show_progress_bar(iteration, total, bar_length=50):\n",
        "        percent = int(round((iteration / total) * 100))\n",
        "        nb_bar_fill = int(round((bar_length * percent) / 100))\n",
        "        bar_fill = '#' * nb_bar_fill\n",
        "        bar_empty = ' ' * (bar_length - nb_bar_fill)\n",
        "        sys.stdout.write(\"\\r  [{0}] {1}%\".format(str(bar_fill + bar_empty), percent))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def _predict(self, model_weights, img_paths):\n",
        "        self.load_model(model_weights)\n",
        "\n",
        "        label_map = self._get_label_map()\n",
        "        img_count = len(img_paths)\n",
        "        y_predictions = np.zeros(img_count, dtype=np.int8)\n",
        "\n",
        "        for i, (img_num, img_path) in enumerate(img_paths):\n",
        "            ModelBase.show_progress_bar(i, img_count)\n",
        "            img_data = self._load_image(img_path)\n",
        "            if isinstance(self.model, Sequential):\n",
        "                pred_index = self.model.predict_classes(img_data)[0]\n",
        "            else:\n",
        "                predictions = self.model.predict(img_data)\n",
        "                pred_index = np.argmax(predictions, axis=1)[0]\n",
        "            pred_label = label_map[pred_index]\n",
        "            y_predictions[img_num-1] = pred_label\n",
        "  \n",
        "        return y_predictions\n",
        "\n",
        "    def _predict_instance(self, model_weights, img_paths):\n",
        "        print('Instance-based predictions...')\n",
        "        self.load_model(model_weights)\n",
        "\n",
        "        # Sort image paths in-place\n",
        "        img_paths.sort(key=lambda tup: tup[0])\n",
        "\n",
        "        # Group images so they come in pairs\n",
        "        instance_pairs = list(zip(*[iter(img_paths)]*2))\n",
        "\n",
        "        label_map = self._get_label_map()\n",
        "        img_count = len(img_paths)\n",
        "        y_predictions = np.zeros(img_count, dtype=np.int8)\n",
        "        \n",
        "        print('Image count={}      instance_pairs count={}'.format(img_count, len(instance_pairs)))\n",
        "\n",
        "        for i, ((img1_num, img1_path),(img2_num, img2_path)) in enumerate(instance_pairs):\n",
        "            ModelBase.show_progress_bar(i, img_count)\n",
        "            \n",
        "            if i < 10:\n",
        "              print('Path1={}    Path2={}'.format(img1_path, img2_path))\n",
        "            \n",
        "            img1_data = m._load_image(img1_path)\n",
        "            img2_data = m._load_image(img2_path)\n",
        "            img1_pred = m.model.predict(img1_data)\n",
        "            img2_pred = m.model.predict(img2_data)\n",
        "\n",
        "            img1_pred_index = np.argmax(img1_pred, axis=1)[0]\n",
        "            img2_pred_index = np.argmax(img2_pred, axis=1)[0]\n",
        "\n",
        "            if img1_pred_index != img2_pred_index:\n",
        "                img1_highest_score = np.max(img1_pred, axis=1)[0]\n",
        "                img2_highest_score = np.max(img2_pred, axis=1)[0]\n",
        "\n",
        "                # if class labels for different views differ,\n",
        "                # we assign to the instance the class label\n",
        "                # with the highest confidence score. \n",
        "                if img1_highest_score > img2_highest_score:\n",
        "                   img2_pred_index = img1_pred_index\n",
        "                else:\n",
        "                   img1_pred_index = img2_pred_index\n",
        "\n",
        "            img1_pred_label = label_map[img1_pred_index]\n",
        "            img2_pred_label = label_map[img2_pred_index]\n",
        "            \n",
        "            if y_predictions[img1_num-1] > 0:\n",
        "              print('Something is wrong with index ', img1_num-1)\n",
        "              break\n",
        "            if y_predictions[img2_num-1] > 0:\n",
        "              print('Something is wrong with index ', img2_num-1)\n",
        "              break\n",
        "            \n",
        "            y_predictions[img1_num-1] = img1_pred_label\n",
        "            y_predictions[img2_num-1] = img2_pred_label\n",
        "        \n",
        "        print('\\n ... predictions done')\n",
        "        return y_predictions\n",
        "      \n",
        "    def _get_label_map(self):\n",
        "        # We need the ImageDataGenerator used to train the model\n",
        "        # because it contains a mapping between classes and indices\n",
        "        train_gen = self._get_train_generator()\n",
        "\n",
        "        # Reverse keys and values so values becomes keys\n",
        "        label_map = {v: int(k) for k, v in train_gen.class_indices.items()}\n",
        "\n",
        "        return label_map\n",
        "\n",
        "    def _load_image(self, image_path):\n",
        "        img = image.load_img(image_path, target_size=(self.img_width, self.img_height))\n",
        "        x = image.img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        return preprocess_input(x)\n",
        "\n",
        "    def _get_test_image_paths(self):\n",
        "        final_list = []\n",
        "        for img_name in os.listdir(self.test_data_dir):\n",
        "            img_number = int(re.findall(r'\\d+', img_name)[0])\n",
        "            img_path = os.path.join(self.test_data_dir, img_name)\n",
        "            final_list.append((img_number, img_path))\n",
        "        return final_list\n",
        "\n",
        "    def _get_validation_image_paths(self):\n",
        "        final_list = []\n",
        "        for sub_dir in os.listdir(self.validation_data_dir):\n",
        "            sub_dir_path = os.path.join(self.validation_data_dir, sub_dir)\n",
        "            for img_name in os.listdir(sub_dir_path):\n",
        "                img_number = int(re.findall(r'\\d+', img_name)[0])\n",
        "                img_path = os.path.join(sub_dir_path, img_name)\n",
        "                final_list.append((img_number, img_path))\n",
        "        return final_list\n",
        "\n",
        "    def _get_validation_generator(self):\n",
        "        return ImageDataGenerator(\n",
        "            preprocessing_function=preprocess_input\n",
        "        ).flow_from_directory(\n",
        "            self.validation_data_dir,\n",
        "            target_size=(self.img_height, self.img_width),\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode=\"categorical\"\n",
        "        )\n",
        "\n",
        "    def _get_train_generator(self):\n",
        "        return ImageDataGenerator(\n",
        "            preprocessing_function=preprocess_input\n",
        "        ).flow_from_directory(\n",
        "            self.train_data_dir,\n",
        "            target_size=(self.img_height, self.img_width),\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode=\"categorical\"\n",
        "        )\n",
        "\n",
        "    def _load_pretrained_weights(self):\n",
        "        saved_weights_path = self._find_saved_weights()\n",
        "        if saved_weights_path is not None:\n",
        "            print('Loading saved weights from: {}'.format(saved_weights_path))\n",
        "            self.model.load_weights(saved_weights_path)\n",
        "\n",
        "        elif self.imagenet_weights_url is not None and len(self.imagenet_weights_url) > 0:\n",
        "            print('Loading imagenet weights...')\n",
        "            model_weights_path = 'saved_weights/{}'.format(os.path.basename(self.imagenet_weights_url))\n",
        "            if os.path.isfile(model_weights_path):\n",
        "                print('Model file already downloaded')\n",
        "            else:\n",
        "                # Download pre-trained weights\n",
        "                print('Downloading {}...'.format(model_weights_path))\n",
        "                urllib.request.urlretrieve(self.imagenet_weights_url, model_weights_path)\n",
        "            self._load_weights_from_file(model_weights_path)\n",
        "        else:\n",
        "            print('No pre-trained weights loaded!')\n",
        "\n",
        "    def _create(self):\n",
        "        raise NotImplementedError('subclasses must override _create()')\n",
        "\n",
        "    def _freeze_top_layers(self):\n",
        "        if self.n_freeze_layers > 1:\n",
        "            print(\"Freezing {} layers\".format(self.n_freeze_layers))\n",
        "            for layer in self.model.layers[:self.n_freeze_layers]:\n",
        "                layer.trainable = False\n",
        "            for layer in self.model.layers[self.n_freeze_layers:]:\n",
        "                layer.trainable = True\n",
        "\n",
        "    def _get_callbacks(self):\n",
        "        # Define model checkpoint\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            'saved_weights/%s-epoch{epoch:02d}-acc{acc:.2f}-loss{loss:.2f}'\n",
        "            '-valacc{val_acc:.2f}-valloss{val_loss:.2f}.hdf5' % self.model_name,\n",
        "            monitor='val_acc',\n",
        "            save_best_only=False,\n",
        "            save_weights_only=True,\n",
        "            mode='auto',\n",
        "            period=1,\n",
        "            verbose=self.verbose\n",
        "        )\n",
        "\n",
        "        # Define early stopping\n",
        "        early_stop = EarlyStopping(\n",
        "            monitor='val_acc',\n",
        "            min_delta=0,\n",
        "            patience=10,\n",
        "            mode='auto',\n",
        "            verbose=self.verbose\n",
        "        )\n",
        "        \n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n",
        "\n",
        "        return [checkpoint, early_stop, reduce_lr]\n",
        "\n",
        "    def _find_saved_weights(self, models_dir='./saved_weights/'):\n",
        "        if not os.path.isdir(models_dir):\n",
        "            return None\n",
        "\n",
        "        list_of_files = sorted(os.listdir(models_dir))\n",
        "        best_model = None\n",
        "        best_acc = 0\n",
        "        for f in list_of_files:\n",
        "            if f.startswith(self.model_name):\n",
        "                values = re.findall('val[^\\d]*(\\d+\\.\\d*)', f)\n",
        "                acc = float(values[0])\n",
        "                if acc > best_acc:\n",
        "                    best_acc = acc\n",
        "                    best_model = os.path.join(models_dir, f)\n",
        "        return best_model\n",
        "\n",
        "    def _load_weights_from_file(self, file_path):\n",
        "        print('Loading weights from {}...'.format(file_path))\n",
        "\n",
        "        layer_indices = {l.name: i for (i, l) in enumerate(self.model.layers)}\n",
        "\n",
        "        # Load weights from the downloaded file\n",
        "        with h5py.File(file_path) as model_weights_file:\n",
        "            layer_names = model_weights_file.attrs['layer_names']\n",
        "            for i, layer_name in enumerate(layer_names):\n",
        "                level_0 = model_weights_file[layer_name]\n",
        "                transferred_weights = []\n",
        "                for k0 in level_0.keys():\n",
        "                    level_1 = level_0[k0]\n",
        "                    if hasattr(level_1, 'keys'):\n",
        "                        for k1 in level_1.keys():\n",
        "                            transferred_weights.append(level_1[k1][()])\n",
        "                    else:\n",
        "                        transferred_weights.append(level_0[k0][()])\n",
        "                if self.imagenet_use_id:\n",
        "                    layer_index = i\n",
        "                else:\n",
        "                    layer_key = layer_name.decode('UTF-8')\n",
        "                    if layer_key not in layer_indices:\n",
        "                        continue\n",
        "                    layer_index = layer_indices[layer_key]\n",
        "                self.model.layers[layer_index].set_weights(transferred_weights)\n",
        "        print('Done loading weights')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5B7yMXM0t1EM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ModelInceptionV3Take2(ModelBase):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        ModelBase.__init__(self, *args, **kwargs)\n",
        "\n",
        "    def _create(self):\n",
        "        base_model = applications.InceptionV3(\n",
        "            include_top=False,\n",
        "            input_shape=(self.img_width, self.img_height, self.img_channels)\n",
        "        )\n",
        "        output_layer = base_model.output\n",
        "        output_layer = GlobalAveragePooling2D(name='avg_pool')(output_layer)\n",
        "        output_layer = Dense(256, activation='relu')(output_layer)\n",
        "        output_layer = Dropout(0.5)(output_layer)\n",
        "        output_layer = Dense(self.n_labels, activation='softmax', name='predictions')(output_layer)\n",
        "        self.model = Model(inputs=base_model.input, outputs=output_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kjUSeP0buAF7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "m = ModelInceptionV3Take2(\n",
        "    model_name='model-15-inception-v3-take3',\n",
        "    n_freeze_layers=0,\n",
        "    batch_size=64,\n",
        "    verbose=1,\n",
        "    epochs=300\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AgRAFGUm5dKp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "6380f8a0-9834-44fa-b5cf-f7a29ce820d3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525887853388,
          "user_tz": -120,
          "elapsed": 156378,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_val_pred = m.predict_validation(model_weights='saved_weights/model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating model...\n",
            "Loading weights from saved_weights/model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5...\n",
            "Compiling...\n",
            "Found 70208 images belonging to 29 classes.\n",
            "  [########                                          ] 17%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  [##################################################] 100%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rbk09z7E_hj_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "y_validation = pd.read_csv('Validation/valLbls.csv', header=None, names=['label'])['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h3O0Woiw6llN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "d5002dfc-38c3-46d5-af09-80bec14507ea",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525887856427,
          "user_tz": -120,
          "elapsed": 720,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_validation, y_val_pred))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             precision    recall  f1-score   support\n",
            "\n",
            "          1       1.00      0.99      1.00       116\n",
            "          2       0.86      0.84      0.85        96\n",
            "          3       0.90      0.96      0.93        94\n",
            "          4       0.99      0.99      0.99        92\n",
            "          5       0.97      0.99      0.98        88\n",
            "          6       0.96      0.83      0.89        92\n",
            "          7       0.89      0.84      0.86        92\n",
            "          8       0.83      0.89      0.86        88\n",
            "          9       0.97      0.99      0.98        88\n",
            "         10       0.96      0.91      0.94        82\n",
            "         11       0.99      0.98      0.98        86\n",
            "         12       0.93      0.96      0.94        80\n",
            "         13       0.98      1.00      0.99        80\n",
            "         14       0.89      0.96      0.92        82\n",
            "         15       0.94      0.90      0.92        82\n",
            "         16       0.94      0.99      0.96        82\n",
            "         17       1.00      0.99      0.99        80\n",
            "         18       0.97      0.97      0.97        80\n",
            "         19       1.00      0.99      0.99        78\n",
            "         20       0.81      0.96      0.88        74\n",
            "         21       0.95      1.00      0.97        76\n",
            "         22       0.96      0.95      0.95        74\n",
            "         23       0.96      0.83      0.89        66\n",
            "         24       0.97      0.94      0.96        72\n",
            "         25       0.72      0.56      0.63        64\n",
            "         26       0.96      0.98      0.97        66\n",
            "         27       0.97      1.00      0.98        56\n",
            "         28       0.93      0.98      0.95        52\n",
            "         29       0.97      0.97      0.97        40\n",
            "\n",
            "avg / total       0.94      0.94      0.94      2298\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oQwroDcm5c_j",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "f3fdae21-aebc-4369-80dd-82bc6e74f130",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525888036404,
          "user_tz": -120,
          "elapsed": 159235,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_val_pred = m.predict_instance_validation(model_weights='saved_weights/model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5')\n",
        "\n",
        "print(classification_report(y_validation, y_val_pred))\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instance-based predictions...\n",
            "Creating model...\n",
            "Loading weights from saved_weights/model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5...\n",
            "Compiling...\n",
            "Found 70208 images belonging to 29 classes.\n",
            "  [########                                          ] 15%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  [#########################                         ] 50% ... predictions done\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          1       1.00      1.00      1.00       116\n",
            "          2       0.91      0.88      0.89        96\n",
            "          3       0.90      0.98      0.94        94\n",
            "          4       1.00      1.00      1.00        92\n",
            "          5       0.98      1.00      0.99        88\n",
            "          6       1.00      0.93      0.97        92\n",
            "          7       0.91      0.91      0.91        92\n",
            "          8       0.81      0.86      0.84        88\n",
            "          9       0.98      1.00      0.99        88\n",
            "         10       0.98      0.98      0.98        82\n",
            "         11       1.00      1.00      1.00        86\n",
            "         12       0.93      0.97      0.95        80\n",
            "         13       0.98      1.00      0.99        80\n",
            "         14       0.89      0.95      0.92        82\n",
            "         15       0.97      0.95      0.96        82\n",
            "         16       1.00      1.00      1.00        82\n",
            "         17       1.00      1.00      1.00        80\n",
            "         18       1.00      1.00      1.00        80\n",
            "         19       1.00      1.00      1.00        78\n",
            "         20       0.88      0.97      0.92        74\n",
            "         21       1.00      1.00      1.00        76\n",
            "         22       0.97      1.00      0.99        74\n",
            "         23       1.00      0.82      0.90        66\n",
            "         24       1.00      0.94      0.97        72\n",
            "         25       0.82      0.56      0.67        64\n",
            "         26       0.94      0.97      0.96        66\n",
            "         27       1.00      1.00      1.00        56\n",
            "         28       0.93      1.00      0.96        52\n",
            "         29       0.95      1.00      0.98        40\n",
            "\n",
            "avg / total       0.96      0.96      0.96      2298\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Kfh9mkv5c75",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "9e587c17-f746-4c24-b0e3-714fa69c651b"
      },
      "cell_type": "code",
      "source": [
        "y_test_pred = m.predict_instance_test(model_weights='saved_weights/model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instance-based predictions...\n",
            "Creating model...\n",
            "Loading weights from saved_weights/model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5...\n",
            "Compiling...\n",
            "Found 70208 images belonging to 29 classes.\n",
            "Image count=3460      instance_pairs count=1730\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image1.jpg    Path2=Test/TestImages/Image2.jpg\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image3.jpg    Path2=Test/TestImages/Image4.jpg\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image5.jpg    Path2=Test/TestImages/Image6.jpg\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image7.jpg    Path2=Test/TestImages/Image8.jpg\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image9.jpg    Path2=Test/TestImages/Image10.jpg\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image11.jpg    Path2=Test/TestImages/Image12.jpg\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image13.jpg    Path2=Test/TestImages/Image14.jpg\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image15.jpg    Path2=Test/TestImages/Image16.jpg\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image17.jpg    Path2=Test/TestImages/Image18.jpg\n",
            "  [                                                  ] 0%Path1=Test/TestImages/Image19.jpg    Path2=Test/TestImages/Image20.jpg\n",
            "  [#                                                 ] 2%"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  [#######                                           ] 14%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MPzrobeW5c40",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_pred_file_path = 'test-pred-model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24-instance-based.csv'\n",
        "ModelBase.write_predictions(y_test_pred, file_name=test_pred_file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h6gkKtU55c15",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "files.download(test_pred_file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YMPam6B45czN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73331f0e-bafe-4099-8cab-6025dd0b30f3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525888684086,
          "user_tz": -120,
          "elapsed": 467,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_test_pred"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0], dtype=int8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "metadata": {
        "id": "8umbHp7O5cwZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VFCjxWEs5cqJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EsuXbmwL5chR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OgXxUJg3uM7S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "image_paths = m._get_validation_image_paths()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VBcRYBP0vrAn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "be44b055-d931-47a8-e306-d515bf688696",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525884214633,
          "user_tz": -120,
          "elapsed": 512,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Sort it in-place\n",
        "image_paths.sort(key=lambda tup: tup[0])\n",
        "\n",
        "instance_pairs = zip(*[iter(image_paths)]*2)\n",
        "\n",
        "for i, ((img1_num, img1_path),(img2_num, img2_path)) in enumerate(instance_pairs):\n",
        "  print('i={}  {} {}'.format(i, img1_num, img2_num))\n",
        "  if i > 10:\n",
        "    break"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i=0  1 2\n",
            "i=1  3 4\n",
            "i=2  5 6\n",
            "i=3  7 8\n",
            "i=4  9 10\n",
            "i=5  11 12\n",
            "i=6  13 14\n",
            "i=7  15 16\n",
            "i=8  17 18\n",
            "i=9  19 20\n",
            "i=10  21 22\n",
            "i=11  23 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nwFHL1XLxYah",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "781d1cf3-93eb-4340-ffd5-7a09109933ae",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525884094940,
          "user_tz": -120,
          "elapsed": 510,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "image_paths[:10]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'Validation/ValidationImages/1/Image1.jpg'),\n",
              " (2, 'Validation/ValidationImages/1/Image2.jpg'),\n",
              " (3, 'Validation/ValidationImages/1/Image3.jpg'),\n",
              " (4, 'Validation/ValidationImages/1/Image4.jpg'),\n",
              " (5, 'Validation/ValidationImages/1/Image5.jpg'),\n",
              " (6, 'Validation/ValidationImages/1/Image6.jpg'),\n",
              " (7, 'Validation/ValidationImages/1/Image7.jpg'),\n",
              " (8, 'Validation/ValidationImages/1/Image8.jpg'),\n",
              " (9, 'Validation/ValidationImages/1/Image9.jpg'),\n",
              " (10, 'Validation/ValidationImages/1/Image10.jpg')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "aagIP25Bvssm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "instance_pairs = zip(*[iter(image_paths)]*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZoZvYeRTv4Du",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "t2 = list(instance_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iz0XLOABv8He",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a9f07256-2d46-46da-dcd6-b542a29eaf91",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525884101287,
          "user_tz": -120,
          "elapsed": 507,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "t2[10]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21, 'Validation/ValidationImages/1/Image21.jpg'),\n",
              " (22, 'Validation/ValidationImages/1/Image22.jpg'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "ghTYe_wB06Yg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "1517379b-984d-45a6-df43-752a9cc8cc4b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525884894752,
          "user_tz": -120,
          "elapsed": 1519,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ls saved_weights/"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5\r\n",
            "myfile.bin\r\n",
            "sentinel\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "48QvAw5I04YJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "efac7553-e94a-4f5a-c1ae-39a3ae686be7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525884932044,
          "user_tz": -120,
          "elapsed": 27635,
          "user": {
            "displayName": "Omar Ali Sheikh-Omar",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "102912719476911650914"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "m.load_model('saved_weights/model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating model...\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 11s 0us/step\n",
            "Loading weights from saved_weights/model-15-inception-v3-take3-epoch55-acc1.00-loss0.02-valacc0.94-valloss0.24.hdf5...\n",
            "Compiling...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f84cdb27710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "ziG7zKIPv-WW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "for i, ((img1_num, img1_path),(img2_num, img2_path)) in enumerate(t2[:10]):\n",
        "    img1_data = m._load_image(img1_path)\n",
        "    img2_data = m._load_image(img2_path)\n",
        "    img1_pred = m.model.predict(img1_data)\n",
        "    img2_pred = m.model.predict(img2_data)\n",
        "\n",
        "    img1_pred_index = np.argmax(img1_pred, axis=1)[0]\n",
        "    img2_pred_index = np.argmax(img2_pred, axis=1)[0]\n",
        "\n",
        "    if img1_pred_index != img2_pred_index:\n",
        "        img1_highest_score = np.max(img1_pred, axis=1)[0]\n",
        "        img2_highest_score = np.max(img2_pred, axis=1)[0]\n",
        "\n",
        "        # if class labels for different views differ,\n",
        "        # we assign to the instance the class label\n",
        "        # with the highest confidence score. \n",
        "        if img1_highest_score > img2_highest_score:\n",
        "           img2_pred_index = img1_pred_index\n",
        "        else:\n",
        "           img1_pred_index = img2_pred_index\n",
        "\n",
        "\n",
        "#     img1_pred_label = label_map[img1_pred_index]\n",
        "#     img2_pred_label = label_map[img2_pred_index]\n",
        "#     y_predictions[img1_num-1] = img1_pred_label\n",
        "#     y_predictions[img2_num-1] = img2_pred_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yA7C0Ws5wjg1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}